{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-cluster\n",
        "!pip install timm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LWAS2jMKIm0",
        "outputId": "325d5507-25c7-4ba4-c072-f21037a2ee6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m740.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.25.2)\n",
            "Building wheels for collected packages: torch-cluster\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp310-cp310-linux_x86_64.whl size=722828 sha256=477e0a39150923a289ac2618b79620b87a59e2039d521b939b02729450dace61\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/78/c3/536637b3cdcc3313aa5e8851a6c72b97f6a01877e68c7595e3\n",
            "Successfully built torch-cluster\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3\n",
            "Collecting timm\n",
            "  Downloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->timm)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 timm-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZWZBCCfEJ53c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch_cluster import fps\n",
        "from timm.models.layers import DropPath\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, context_dim=None):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.norm_context = nn.LayerNorm(context_dim) if context_dim is not None else None\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.norm_context is not None:\n",
        "            context = kwargs['context']\n",
        "            normed_context = self.norm_context(context)\n",
        "            kwargs.update(context=normed_context)\n",
        "\n",
        "        return self.fn(x, **kwargs)\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim=-1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4, drop_path_rate=0.0):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.drop_path(self.net(x))\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, drop_path_rate=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = context_dim if context_dim is not None else query_dim\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "        h = self.heads\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = context if context is not None else x\n",
        "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "\n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "        return self.drop_path(self.to_out(out))\n",
        "\n",
        "class PointEmbed(nn.Module):\n",
        "    def __init__(self, hidden_dim=48, dim=128):\n",
        "        super().__init__()\n",
        "        assert hidden_dim % 6 == 0\n",
        "        self.embedding_dim = hidden_dim\n",
        "        e = torch.pow(2, torch.arange(self.embedding_dim // 6)).float() * np.pi\n",
        "        e = torch.stack([\n",
        "            torch.cat([e, torch.zeros(self.embedding_dim // 6), torch.zeros(self.embedding_dim // 6)]),\n",
        "            torch.cat([torch.zeros(self.embedding_dim // 6), e, torch.zeros(self.embedding_dim // 6)]),\n",
        "            torch.cat([torch.zeros(self.embedding_dim // 6), torch.zeros(self.embedding_dim // 6), e]),\n",
        "        ])\n",
        "        self.register_buffer('basis', e)  # 3 x 16\n",
        "        self.mlp = nn.Linear(self.embedding_dim + 3, dim)\n",
        "\n",
        "    @staticmethod\n",
        "    def embed(input, basis):\n",
        "        projections = torch.einsum('bnd,de->bne', input, basis)\n",
        "        embeddings = torch.cat([projections.sin(), projections.cos()], dim=2)\n",
        "        return embeddings\n",
        "\n",
        "    def forward(self, input):\n",
        "        embed = self.mlp(torch.cat([self.embed(input, self.basis), input], dim=2))\n",
        "        return embed\n",
        "\n"
      ],
      "metadata": {
        "id": "B8b_RtBUJ9sd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ksODB4pyJuTx"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, depth=24, dim=512, queries_dim=512, output_dim=1, num_inputs=2048, num_latents=512, heads=8, dim_head=64, weight_tie_layers=False, decoder_ff=False):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_latents = num_latents\n",
        "\n",
        "        # Initialize cross-attention blocks with a list of two modules: attention and feedforward\n",
        "        self.cross_attend_blocks = nn.ModuleList([\n",
        "            PreNorm(dim, Attention(dim, dim, heads=1, dim_head=dim), context_dim=dim),\n",
        "            PreNorm(dim, FeedForward(dim))\n",
        "        ])\n",
        "\n",
        "        # Initialize point embedding\n",
        "        self.point_embed = PointEmbed(dim=dim)\n",
        "\n",
        "        # Define functions for creating latent attention and feedforward layers with cached function calls\n",
        "        get_latent_attn = lambda: PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, drop_path_rate=0.1))\n",
        "        get_latent_ff = lambda: PreNorm(dim, FeedForward(dim, drop_path_rate=0.1))\n",
        "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
        "\n",
        "        # Initialize a list of layers for the autoencoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        cache_args = {'_cache': weight_tie_layers}\n",
        "\n",
        "        # Create the specified number of layers in the autoencoder\n",
        "        for i in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                get_latent_attn(**cache_args),\n",
        "                get_latent_ff(**cache_args)\n",
        "            ]))\n",
        "\n",
        "        # Initialize decoder cross-attention and feedforward layers\n",
        "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, dim, heads=1, dim_head=dim), context_dim=dim)\n",
        "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
        "\n",
        "        # Output layer for the autoencoder\n",
        "        self.to_outputs = nn.Linear(queries_dim, output_dim) if output_dim is not None else nn.Identity()\n",
        "\n",
        "    def encode(self, pc):\n",
        "        B, N, D = pc.shape\n",
        "        assert N == self.num_inputs\n",
        "\n",
        "        # Flatten the input point cloud\n",
        "        flattened = pc.view(B * N, D)\n",
        "        batch = torch.arange(B).to(pc.device)\n",
        "        batch = torch.repeat_interleave(batch, N)\n",
        "        pos = flattened\n",
        "\n",
        "        # Farthest point sampling (fps) to select a subset of input points\n",
        "        ratio = 1.0 * self.num_latents / self.num_inputs\n",
        "        idx = fps(pos, batch, ratio=ratio)\n",
        "        sampled_pc = pos[idx].view(B, -1, 3)\n",
        "\n",
        "        # Embed the sampled point cloud and the original point cloud\n",
        "        sampled_pc_embeddings = self.point_embed(sampled_pc)\n",
        "        pc_embeddings = self.point_embed(pc)\n",
        "\n",
        "        # Apply cross-attention and feedforward layers to the embeddings\n",
        "        cross_attn, cross_ff = self.cross_attend_blocks\n",
        "        x = cross_attn(sampled_pc_embeddings, context=pc_embeddings, mask=None) + sampled_pc_embeddings\n",
        "        x = cross_ff(x) + x\n",
        "\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, queries):\n",
        "        # Loop through each layer in the autoencoder\n",
        "        for self_attn, self_ff in self.layers:\n",
        "            # Apply latent attention and feedforward layers\n",
        "            x = self_attn(x) + x\n",
        "            x = self_ff(x) + x\n",
        "\n",
        "        # Embed the decoder queries\n",
        "        queries_embeddings = self.point_embed(queries)\n",
        "\n",
        "        # Apply decoder cross-attention to the embeddings and the encoded input\n",
        "        latents = self.decoder_cross_attn(queries_embeddings, context=x)\n",
        "\n",
        "        # Optionally apply decoder feedforward layer\n",
        "        if self.decoder_ff is not None:\n",
        "            latents = latents + self.decoder_ff(latents)\n",
        "\n",
        "        # Return the output logits\n",
        "        return self.to_outputs(latents)\n",
        "\n",
        "    def forward(self, pc, queries):\n",
        "        # Encode the input point cloud\n",
        "        x = self.encode(pc)\n",
        "        # Decode the latent representation with decoder queries\n",
        "        o = self.decode(x, queries).squeeze(-1)\n",
        "        # Return the output logits\n",
        "        return {'logits': o}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "autoencoder = AutoEncoder()\n",
        "\n",
        "# Generate some example input data\n",
        "input_data = torch.randn(1, 2048, 3)  # Batch size 1, 2048 points, 3 dimensions\n",
        "\n",
        "# Generate some example query data for the decoder\n",
        "queries = torch.randn(1, 512, 3)  # Batch size 1, 512 queries, 3 dimensions\n",
        "\n",
        "# Perform a forward pass through the autoencoder\n",
        "output = autoencoder(input_data, queries)\n",
        "\n",
        "# Access the output logits\n",
        "logits = output['logits']"
      ],
      "metadata": {
        "id": "F-k-IPulJx-F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BKzsOvHLPhRE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}