{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0PVqAkFoMRY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from dalle2_pytorch import CLIP\n",
        "\n",
        "clip = CLIP(\n",
        "    dim_text = 512,\n",
        "    dim_image = 512,\n",
        "    dim_latent = 512,\n",
        "    num_text_tokens = 49408,\n",
        "    text_enc_depth = 1,\n",
        "    text_seq_len = 256,\n",
        "    text_heads = 8,\n",
        "    visual_enc_depth = 1,\n",
        "    visual_image_size = 256,\n",
        "    visual_patch_size = 32,\n",
        "    visual_heads = 8,\n",
        "    use_all_token_embeds = True,            # whether to use fine-grained contrastive learning (FILIP)\n",
        "    decoupled_contrastive_learning = True,  # use decoupled contrastive learning (DCL) objective function, removing positive pairs from the denominator of the InfoNCE loss (CLOOB + DCL)\n",
        "    extra_latent_projection = True,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)\n",
        "    use_visual_ssl = True,                  # whether to do self supervised learning on images\n",
        "    visual_ssl_type = 'simclr',             # can be either 'simclr' or 'simsiam', depending on using DeCLIP or SLIP\n",
        "    use_mlm = False,                        # use masked language learning (MLM) on text (DeCLIP)\n",
        "    text_ssl_loss_weight = 0.05,            # weight for text MLM loss\n",
        "    image_ssl_loss_weight = 0.05            # weight for image self-supervised learning loss\n",
        ").cuda()\n",
        "\n",
        "# mock data\n",
        "\n",
        "text = torch.randint(0, 49408, (4, 256)).cuda()\n",
        "images = torch.randn(4, 3, 256, 256).cuda()\n",
        "\n",
        "\"\"\"\n",
        "Explanation:\n",
        "\n",
        "torch.randint(0, 10000, (4, 256)):\n",
        "Generates random integers between 0 and 9999 to simulate a batch of text inputs.\n",
        "The shape (4, 256) indicates a batch size of 4 and a sequence length of 256 tokens.\n",
        "\n",
        "\n",
        "torch.randn(4, 3, 256, 256):\n",
        "Generates random tensors of shape (4, 3, 256, 256) to simulate a batch of images.\n",
        "This represents 4 images with 3 channels (RGB) and each image having dimensions of 256x256 pixels.\n",
        "\"\"\"\n",
        "\n",
        "# train\n",
        "\n",
        "loss = clip(\n",
        "    text,\n",
        "    images,\n",
        "    return_loss = True              # needs to be set to True to return contrastive loss\n",
        ")\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "# do the above with as many texts and images as possible in a loop"
      ]
    }
  ]
}