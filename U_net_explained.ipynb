{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### What is a U-Net?\n",
        "\n",
        "U-Net is a type of deep learning model specifically designed for image segmentation tasks. Image segmentation involves dividing an image into different parts or segments, often to identify and isolate objects or regions of interest within the image. For example, in medical imaging, it can be used to segment different tissues, organs, or anomalies.\n",
        "\n",
        "### Structure of a U-Net\n",
        "\n",
        "The U-Net architecture is named for its U-shaped structure, which consists of two main parts: the contracting path (encoder) and the expansive path (decoder).\n",
        "\n",
        "1. **Contracting Path (Encoder)**:\n",
        "   - The encoder is similar to a traditional convolutional neural network (CNN).\n",
        "   - It consists of several convolutional layers followed by max-pooling layers.\n",
        "   - The purpose of the encoder is to capture the context and features of the input image by progressively reducing its spatial dimensions while increasing the number of feature channels.\n",
        "   - This path captures \"what\" is in the image.\n",
        "\n",
        "2. **Expansive Path (Decoder)**:\n",
        "   - The decoder upsamples the feature maps to the original image size.\n",
        "   - It consists of up-convolutional (transposed convolution) layers followed by convolutional layers.\n",
        "   - The purpose of the decoder is to construct a detailed segmentation map using the features captured by the encoder.\n",
        "   - Skip connections are used to transfer information from the encoder layers directly to the decoder layers, which helps in preserving spatial information and improving segmentation accuracy.\n",
        "   - This path helps in determining \"where\" the objects are located in the image.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Skip Connections**: These connections between corresponding layers in the encoder and decoder help retain high-resolution features that might otherwise be lost during downsampling. They allow the model to use both global and local context for better segmentation.\n",
        "  \n",
        "- **Symmetry**: The U-Net is symmetric, meaning the layers in the decoder mirror those in the encoder. This symmetry aids in the reconstruction of the segmented image.\n",
        "  \n",
        "- **Localization and Context**: U-Net's design allows it to understand both the precise localization of objects and the broader context of the entire image, making it effective for segmentation tasks.\n",
        "\n",
        "### Applications\n",
        "\n",
        "U-Net was originally developed for biomedical image segmentation but has since been widely adopted for various other applications, including:\n",
        "\n",
        "- **Medical Imaging**: Segmenting different tissues, organs, or tumors in medical scans like MRI or CT images.\n",
        "- **Satellite Imagery**: Segmenting different land types, water bodies, and built-up areas.\n",
        "- **Autonomous Driving**: Identifying different objects on the road such as cars, pedestrians, and lanes.\n",
        "- **Agriculture**: Segmenting crops and plants from background soil or other objects.\n"
      ],
      "metadata": {
        "id": "oF2Nv5JjiiHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmwrm9naiNTQ",
        "outputId": "828a7b0e-0788-45f6-9b7e-9d4431b32b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: torch.Size([1, 3, 128, 128])\n",
            "After enc1: torch.Size([1, 64, 128, 128])\n",
            "After enc2: torch.Size([1, 128, 64, 64])\n",
            "After enc3: torch.Size([1, 256, 32, 32])\n",
            "After enc4: torch.Size([1, 512, 16, 16])\n",
            "After bottleneck: torch.Size([1, 1024, 8, 8])\n",
            "After dec4: torch.Size([1, 512, 16, 16])\n",
            "After dec3: torch.Size([1, 256, 32, 32])\n",
            "After dec2: torch.Size([1, 128, 64, 64])\n",
            "After dec1: torch.Size([1, 64, 128, 128])\n",
            "Output: torch.Size([1, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# U-Net Model from Scratch in PyTorch\n",
        "# ===================================\n",
        "\n",
        "# Step 1: Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Step 2: Define the U-Net Model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_channels, out_channels):\n",
        "            block = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "            return block\n",
        "\n",
        "        def upconv_block(in_channels, out_channels):\n",
        "            block = nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "            return block\n",
        "\n",
        "        self.enc1 = conv_block(3, 64)\n",
        "        self.enc2 = conv_block(64, 128)\n",
        "        self.enc3 = conv_block(128, 256)\n",
        "        self.enc4 = conv_block(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = conv_block(512, 1024)\n",
        "\n",
        "        self.upconv4 = upconv_block(1024, 512)\n",
        "        self.dec4 = conv_block(1024, 512)\n",
        "        self.upconv3 = upconv_block(512, 256)\n",
        "        self.dec3 = conv_block(512, 256)\n",
        "        self.upconv2 = upconv_block(256, 128)\n",
        "        self.dec2 = conv_block(256, 128)\n",
        "        self.upconv1 = upconv_block(128, 64)\n",
        "        self.dec1 = conv_block(128, 64)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"Input: {x.shape}\")\n",
        "\n",
        "        enc1 = self.enc1(x)\n",
        "        print(f\"After enc1: {enc1.shape}\")\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        print(f\"After enc2: {enc2.shape}\")\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        print(f\"After enc3: {enc3.shape}\")\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "        print(f\"After enc4: {enc4.shape}\")\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "        print(f\"After bottleneck: {bottleneck.shape}\")\n",
        "\n",
        "        dec4 = self.dec4(torch.cat((self.upconv4(bottleneck), enc4), dim=1))\n",
        "        print(f\"After dec4: {dec4.shape}\")\n",
        "        dec3 = self.dec3(torch.cat((self.upconv3(dec4), enc3), dim=1))\n",
        "        print(f\"After dec3: {dec3.shape}\")\n",
        "        dec2 = self.dec2(torch.cat((self.upconv2(dec3), enc2), dim=1))\n",
        "        print(f\"After dec2: {dec2.shape}\")\n",
        "        dec1 = self.dec1(torch.cat((self.upconv1(dec2), enc1), dim=1))\n",
        "        print(f\"After dec1: {dec1.shape}\")\n",
        "\n",
        "        output = torch.sigmoid(self.final_conv(dec1))\n",
        "        print(f\"Output: {output.shape}\")\n",
        "\n",
        "        return output\n",
        "\n",
        "# Step 3: Create Synthetic Data\n",
        "def create_synthetic_data(input_shape, num_samples):\n",
        "    X = np.random.rand(num_samples, *input_shape).astype(np.float32)\n",
        "    X = np.transpose(X, (0, 3, 1, 2))  # Convert to (N, C, H, W)\n",
        "    return torch.tensor(X)\n",
        "\n",
        "input_shape = (128, 128, 3)  # Example input shape\n",
        "num_samples = 1  # Only one sample to show the tensor sizes\n",
        "\n",
        "X = create_synthetic_data(input_shape, num_samples)\n",
        "\n",
        "# Step 4: Instantiate the Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNet().to(device)\n",
        "\n",
        "# Step 5: Run a Forward Pass\n",
        "test_input = X.to(device)\n",
        "predicted_output = model(test_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lcwZgK3RiN2H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}