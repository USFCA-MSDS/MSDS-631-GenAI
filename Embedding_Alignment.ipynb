{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpWrKqVD5Gyc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Constants\n",
        "EMBED_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Download GloVe and Flickr8k datasets\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip Flickr8k_Dataset.zip\n",
        "!unzip Flickr8k_text.zip\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(filepath):\n",
        "    embeddings_index = {}\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "glove_path = 'glove.6B.300d.txt'\n",
        "embeddings_index = load_glove_embeddings(glove_path)\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_caption(caption):\n",
        "    tokens = word_tokenize(caption.lower())\n",
        "    return [embeddings_index[token] for token in tokens if token in embeddings_index]\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, image_folder, captions_file, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.captions = self.load_captions(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def load_captions(self, file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        captions = {}\n",
        "        for line in lines:\n",
        "            tokens = line.split('\\t')\n",
        "            image_id, caption = tokens[0], tokens[1]\n",
        "            if image_id not in captions:\n",
        "                captions[image_id] = []\n",
        "            captions[image_id].append(caption)\n",
        "        return captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = list(self.captions.keys())[idx]\n",
        "        caption = self.captions[image_id][0]  # Use the first caption\n",
        "        image_path = os.path.join(self.image_folder, image_id)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        caption_embedding = preprocess_caption(caption)\n",
        "        return image, torch.tensor(caption_embedding, dtype=torch.float)\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "image_folder = 'Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "captions_file = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "dataset = FlickrDataset(image_folder, captions_file, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Define the Model\n",
        "class SimpleAlignModel(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(SimpleAlignModel, self).__init__()\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_dim)\n",
        "        self.text_fc = nn.Linear(300, embed_dim)\n",
        "\n",
        "    def forward(self, text, image):\n",
        "        text_embed = F.normalize(self.text_fc(text), dim=-1)\n",
        "        image_embed = F.normalize(self.resnet(image), dim=-1)\n",
        "        return text_embed, image_embed\n",
        "\n",
        "# Initialize Model\n",
        "model = SimpleAlignModel(embed_dim=EMBED_DIM)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, captions in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        batch_size = images.size(0)\n",
        "\n",
        "        # Flatten the caption embeddings to (batch_size, 300)\n",
        "        captions = captions.view(batch_size, -1, 300).mean(dim=1)\n",
        "\n",
        "        text_embed, image_embed = model(captions, images)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        similarity_matrix = torch.matmul(text_embed, image_embed.t())\n",
        "\n",
        "        # Compute loss (contrastive loss using labels as ground truth)\n",
        "        labels = torch.arange(similarity_matrix.size(0)).to(similarity_matrix.device)\n",
        "        loss = criterion(similarity_matrix, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {total_loss / len(dataloader):.4f}')\n",
        "\n",
        "print(\"Training completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "1. **Data Preparation:**\n",
        "   - **GloVe Embeddings:** We download and load the GloVe word embeddings.\n",
        "   - **Flickr8k Dataset:** We download and unzip the Flickr8k dataset.\n",
        "   - **Preprocessing:** Captions are tokenized and converted to embeddings using GloVe. Images are transformed using standard preprocessing steps.\n",
        "\n",
        "2. **Model Definition:**\n",
        "   - **ResNet:** We use a pretrained ResNet50 model to extract image embeddings, modifying the final layer to output the desired embedding dimension.\n",
        "   - **Text Embeddings:** A simple linear layer maps the GloVe embeddings to the common embedding space.\n",
        "\n",
        "3. **Training Loop:**\n",
        "   - **Forward Pass:** We compute embeddings for both text and images.\n",
        "   - **Similarity Matrix:** A similarity matrix is computed using the dot product of text and image embeddings.\n",
        "   - **Loss Calculation:** We use cross-entropy loss to align the embeddings.\n",
        "   - **Backpropagation:** The loss is backpropagated, and the model parameters are updated.\n",
        "\n"
      ],
      "metadata": {
        "id": "oQRM0DT-5flt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tINlW6N55rNf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}